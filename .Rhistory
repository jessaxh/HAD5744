Dataset1 <- dummy_cols(Dataset1, select_columns = "continent") #used fast dummies to make dummy variables
continet_regression <- lm(LEBF20052~log(HXPC2005)+log(GDPPCUS2005) + continent_Africa +
continent_Americas + continent_Asia + continent_Europe
+ continent_Oceania, Dataset1)
#all dummy variables are in the regression - to let R decide which one to drop
Regression_table4 <- msummary(list("continent_reg"=continet_regression),
stars=c('*' = .1, '**' = .05, '***' = .01),
fmt = 5,  statistic = c("conf.int",  "standard error = {std.error}",
"p-value = {p.value}"))
Regression_table4
interaction_regression <- lm(LEBF20052~log(HXPC2005)
+log(GDPPCUS2005)
+ continent_Africa
+ continent_Americas
+ continent_Asia
+ continent_Europe
+ continent_Oceania
+ log(HXPC2005):continent_Africa, Dataset1)
Regression_table5 <- msummary(list("interaction_reg"=interaction_regression),
stars=c('*' = .1, '**' = .05, '***' = .01),
fmt = 5,  statistic = c("conf.int",
"standard error = {std.error}",
"p-value = {p.value}"))
Regression_table5
q.9_regression.1 <- lm(LEBF20052~log(GDPPCUS2005), Dataset1)
q.9_regression.2 <- lm(LEBF20052~log(HXPC2005), Dataset1)
q.9_table <- msummary(list("LEBF_log(GDP)"=q.9_regression.1, "LEBF_log(HXPC)"q.9_regression.2,"LEBF_log(GDP)+log(HXPC)" third_regression), fmt = "%.4f",
q.9_regression.1 <- lm(LEBF20052~log(GDPPCUS2005), Dataset1)
q.9_regression.2 <- lm(LEBF20052~log(HXPC2005), Dataset1)
q.9_table <- msummary(list("LEBF_log(GDP)"=q.9_regression.1, "LEBF_log(HXPC)"=q.9_regression.2,"LEBF_log(GDP)+log(HXPC)"= third_regression), fmt = "%.4f",
stars=c('*' = .1, '**' = .05, '***' = .01),  statistic = c("conf.int",
"standard error =
{std.error}",
"p-value = {p.value}"))
#added pvalues
q.9_table
q.9_regression.1 <- lm(LEBF20052~log(GDPPCUS2005), Dataset1)
q.9_regression.2 <- lm(LEBF20052~log(HXPC2005), Dataset1)
q.9_table <- msummary(list("LEBF_log(GDP)"=q.9_regression.1, "LEBF_log(HXPC)"=q.9_regression.2,
"LEBF_log(GDP)+log(HXPC)"= third_regression), fmt = "%.4f",
stars=c('*' = .1, '**' = .05, '***' = .01),
statistic = c("conf.int", "standard error = {std.error}",
"p-value = {p.value}"))
#added pvalues
q.9_table
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # load the installed package for each new session of R
library(broom) # helps for storing regression output
library(here) # for file organization
library(modelsummary) # For making regression tables
library(psych) # For creating summary statistics table
library(countrycode) # Used to get continents from countries
library(fastDummies) # To create dummy variables
library(vtable)
library(lmtest)
# Read in cleaned data
Dataset1 <- read.csv("Dataset 1_Clean.csv")
# Convert to numerics (aside from Country)
Dataset1[, 2:19] <- lapply(Dataset1[, 2:19], as.numeric)
third_regression <- lm(LEBF20052~log(HXPC2005)+log(GDPPCUS2005), Dataset1)
autocorrelation_se<- msummary( third_regression,
stars=c('*' = .1, '**' = .05, '***' = .01), vcov ="Neweywest",
fmt = "%.4f")
autocorrelation_se
fmt = "%.4f")
autocorrelation_se<- msummary(list("autocorrelation_corr" =third_regression),
stars=c('*' = .1, '**' = .05, '***' = .01), vcov ="Neweywest",
fmt = "%.4f")
autocorrelation_se
robust_se<- msummary(list("robust_se" =third_regression),
stars=c('*' = .1, '**' = .05, '***' = .01), vcov ="Neweywest",
fmt = "%.4f")
robust_se
autocorrelation_se<- msummary(list("autocorrelation_corr" =third_regression),
stars=c('*' = .1, '**' = .05, '***' = .01), vcov ="Neweywest",
fmt = "%.4f")
robust_se<- msummary(list("robust_se" =third_regression),
stars=c('*' = .1, '**' = .05, '***' = .01), vcov ="robust",
fmt = "%.4f")
robust_se
autocorrelation_se<- msummary(list("autocorrelation_corr" =third_regression),
stars=c('*' = .1, '**' = .05, '***' = .01), vcov ="Neweywest",
fmt = "%.4f")
autocorrelation_se
robust_se
robust_se<- msummary(list("robust_se" =third_regression, "autocorrelation_se" = third_regression),
stars=c('*' = .1, '**' = .05, '***' = .01), vcov =c("robust", "Neweywest"),
fmt = "%.4f")
robust_se
se_corrections<- msummary(list("robust_se" =third_regression,
"autocorrelation_se" = third_regression),
stars=c('*' = .1, '**' = .05, '***' = .01), vcov =c("robust", "Neweywest"),
fmt = "%.4f")
se_corrections
plot(third_regression)
plot(third_regression)
#plot the re
par(mfrow = c(2, 2))
se_corrections
plot(third_regression)
se_corrections
#specification using both robust corrections for standard errors and autocorrelation
se_corrections<- msummary(list("homoscedastic" = third_regression,
"robust_se" =third_regression,
"autocorrelation_se" = third_regression),
stars=c('*' = .1, '**' = .05, '***' = .01), vcov =c("classical","robust", "Neweywest"),
fmt = "%.4f")
se_corrections
knitr::opts_chunk$set(echo = TRUE)
here()
knitr::opts_chunk$set(echo = TRUE)
name <- Sys.info()
name[7]
### Load the packages we will need for this file ####
library(tidyverse) # load the installed package for each new session of R
library(broom) # helps for storing regression output
library(here) # for file organization
library(modelsummary) # For making regression tables
library(psych) # For creating summary statistics table
library(countrycode) # Used to get continents from countries
library(fastDummies) # To create dummy variables
library(vtable) #Shows information about the variables in your data set
library(lmtest) #A collection of tests, data sets, and examples for diagnostic checking in linear regression models
here()
assign_data <- read.csv("Dataset 1.csv") # assign data to an object
assign_data <- read.csv("Dataset 1.csv") # assign data to an object
for(values in names(assign_data)){
print(names(assign_data[values]))
name <- names(assign_data[values])
print(class(assign_data[,values]))
}
# Convert all to character
assign_data[, 1:19] <- lapply(assign_data[, 1:19], as.character)
# Assign the NAs in dataset to NA
assign_data <- mutate_at(.tbl = assign_data,
.vars = vars(Country:PctPop65Pl2005),
.funs = ~case_when(. == "#N/A" ~ NA_character_,
. == "-" ~ NA_character_,
TRUE ~ .,
TRUE ~ as.character(NA_character_)))
write.csv(assign_data, "Dataset 1_Clean.csv", row.names = FALSE)
Dataset1 <- read.csv("Dataset 1_Clean.csv")
Dataset1[, 2:19] <- lapply(Dataset1[, 2:19], as.numeric)
Dataset1 <- read.csv("Dataset 1_Clean.csv")
# Convert to numerics (aside from Country)
Dataset1[, 2:19] <- lapply(Dataset1[, 2:19], as.numeric)
---
title: "Assignment 1"
name: Husayn Jessa and Aidan Bodner
date: '2022-10-05'
output:
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r Assignment Intro}
name <- Sys.info()
name[7]
### Load the packages we will need for this file ####
library(tidyverse) # load the installed package for each new session of R
library(broom) # helps for storing regression output
library(here) # for file organization
library(modelsummary) # For making regression tables
library(psych) # For creating summary statistics table
library(countrycode) # Used to get continents from countries
library(fastDummies) # To create dummy variables
library(vtable) #Shows information about the variables in your data set
library(lmtest) #A collection of tests, data sets, and examples for diagnostic checking in linear regression models
# set working directory
here()
assign_data <- read.csv("Dataset 1.csv") # assign data to an object
```
--------------------------------------------------------------------------------
```{r Clean Data}
# Scan through dataframe to see variable class types
for(values in names(assign_data)){
print(names(assign_data[values]))
name <- names(assign_data[values])
print(class(assign_data[,values]))
}
# Convert all to character
assign_data[, 1:19] <- lapply(assign_data[, 1:19], as.character)
# Assign the NAs in dataset to NA
assign_data <- mutate_at(.tbl = assign_data,
.vars = vars(Country:PctPop65Pl2005),
.funs = ~case_when(. == "#N/A" ~ NA_character_,
. == "-" ~ NA_character_,
TRUE ~ .,
TRUE ~ as.character(NA_character_)))
# Save cleaned data
write.csv(assign_data, "Dataset 1_Clean.csv", row.names = FALSE)
# Read in cleaned data
Dataset1 <- read.csv("Dataset 1_Clean.csv")
# Convert to numerics (aside from Country)
Dataset1[, 2:19] <- lapply(Dataset1[, 2:19], as.numeric)
```
--------------------------------------------------------------------------------
Question 1: Draw a preliminary DAG suggesting a relationship between the following variables and LEBF: gross domestic product per capita (GDPPC), health expenditure per capita (HXPC), and total fertility rate. Include any other covariates in the data set you think are relevant (note that you do not have to include all of the variables in the data). Justify your DAG with text.
[Insert "Table 1: Initial Variables Included in DAG"]
[Insert "Table 2: Final Variables Included in DAG"]
[Insert "FINAL DAG"]
Our DAG begins with a pathway with Country and Age. Age is a derived variable, collapsing PctPop0142005 and PctPop65Pl2005. For a given country, there will be a different age distribution, thus we believe this to be the first relationship in the pathway.
From Age, there are three new pathways, one to GDPPC, one to PctUrb, and one to a derived variable TotFertRate. As the distribution of age within a country will determine the proportion of adults able to participate within the workforce and contribute to economic productivity, we propose a pathway from Age through GDPPC. Next, we propose a pathway from Age to PctUrb as the proportion of younger people in a country will likely determine the percentage of the population that is urban dwelling as younger people are typically drawn to urban centres to pursue economic, social, and educational opportunities. Next, the pathway between Age and TotFertRate is a pathway to another derived variable (TotFertRate). TotFertRate collapses AdolFertRt to simplify the causal pathway through TotFertRate. We propose this relationship as countries with greater proportion of young people will likely see a higher total fertility rate.
Moving to GDPPC, there are four pathways leading away from the variable: to FtoMPrimaryEnrl, HXPC, UtilityInfr, and TotFertRate.  The pathway from GDPPC to FtoMPrimaryEnrl is proposed as economic output will likely increase overall educational resources within a country giving women/girls greater opportunity to receive a primary school education. Next, the pathway between GDPPC and HXPC. HXPC collapses PctHXPUB2005 within it to indicate overall health expenditure within a given country, thereby simplifying the interpretation of the pathways. The pathway is proposed as GDPPC will determine a country's overall spending capacity, including spending on the healthcare system. Next, the pathway between GDPPC and UtilityInfr. UtilityInfr is also a derived variable, collapsing Sanitation2005ONY, ImprWaterUrb2005ONY, and ImprWaterRur2005ONY. This derived variable subsumes the beforementioned variables to simplify understanding of the pathway between economic output and a country's utility infrastructure (in this case water and sanitation). We propose this relationship in a similar vein to that between GDPPC and HXPC, the extent of GDPPC will determine the extent that a country can attain and maintain utility based infrastructure. Next, the relationship between GDPPC and TotFertRate is proposed as higher GDPPC nations will likely see a general increase in the cost of raising children, leading to a decrease in the proportion of newborns. Similarly, higher GDPPC would suggest greater economic independence for women, minimizing traditional domestic roles.
Moving to PctUrb, there are three pathways leading away from the variable to GDPPC, FtoMPrimaryEnrl, and HXPC. The pathway between PctUrb and GDPPC is proposed as nations with a higher percentage of urban residents would be assumed to have higher economic activity. Next, PctUrb to FtoMPrimaryEnrl is proposed as nations with a higher percentage of urban residents likely have greater centralization of educational resources, and thus more women/girls who can access those resources. Next, PctUrb to HXPC is proposed as nations with a higher percentage of urban residents may see more health related issues due to urbanization (crime, disease spread, resource allocation) - although health resources may be more centralized in urban centres - influencing the extent of healthcare spending.
Moving to FtoMPrimaryEnrl, the pathway continues on from PctUrb with further relationships proposed onto FLPR and TotFertRate. The pathway between FtoMPrimaryEnrl and FLPR is proposed as it is expected that the more women who receive education the more they will be able to participate in the workforce. There is a furhter pathway directly to TotFertRate where we expect greater education to generally lead to lower fertility due to greater knowledge regarding family planning and sexual health. Similarly, we propose FLPR as a mediator through an indirect pathway from FtoMPrimaryEnrl through FLPR to TotFertRate where we would expect workforce involvement to result in greater financial independence and greater ability for women to negotiate sex.
Moving to TotFertRate, we suggest that the previously mentioned pathways from FtoMPrimaryEnrl, FLPR, GDPPC, and Age all are associated with TotFertRate, which then is ultimately associated with the extent of maxINFM. We propose that TotFertRate is associated with maxINFM as the more infants that are born the greater the likelihood that there may be mortality at birth.
Moving to UtilityInfr, we believe there are three pathways leading away from the variable, towards HXPC, maxINFM, and LEBF. We propose a pathway from UtilityInfr to HXPC as the likelihood of water born disease is increased as the amount and quality of utility infrastructure decreases, leading to greater healthcare spending to account for a greater morbidity burden. Next, we propose a pathway towards maxINFM for similar reasons, lack of clean water and sanitation will likely lead directly to infant mortality. Next, we suggest a direct pathway to LEBF as lack of clean water and sanitation will likely lead directly to mortality for women.
Moving to HXPC, there are pathways from PctUrb, GDPPC, UtilityInfr leading through the variable. We propose that HXPC is associated with maxINFM as well as directly with LEBF. The pathway with maxINFM is suggested as higher health expenditures generally indicates more health resources, allowing for more infants to receive healthcare services, thereby reducing infant mortality. The pathway between HXPC and LEBF is proposed as as higher health expenditures generally indicates more health resources, allowing for more women to receive healthcare services, increasing life expectancy for women.
Lastlly, the pathway from maxINFM to LEBF is suggested as an externality of infant mortality during childbirth is often maternal mortality, indicating the higher the infant mortality, the greater the likelihood for maternal mortality, thereby decreasing female life-expectancy.
--------------------------------------------------------------------------------
Question 2: Make sure that your DAG includes relationships between independent variables in (1), if needed. What does your DAG tell you about interpreting any regression coefficients (between LEBF, GDPPC, and HXPC) causally?
A DAG can allow for causal interpretations of regression coefficient if all the back door paths
represented in the DAG are closed and if the assumptions that are made to design and create the
relationships within the DAG are correct (all the variable positions in the DAG are justified and
how their relationships are depicted are correct). When examining our DAG, we are not able to
interpret our regression coefficients causally because we are unable to confirm that we have made
the correct assumptions within our DAG nor if they are the right ones to depict the relationship at hand. More specifically, in terms of the relationships between LEBF, GDPPC, and HXPC, due to the number of potential pathways leading towards LEBF that are not from GDPPC or HXPC, it would be unwise to assume a causal relationship between either GDPPC or HXPC and LEBF.
--------------------------------------------------------------------------------
Question 3: Make a table providing summary statistics for the variables listed in (1). Your table should include the mean, standard deviation, and sample size for each variable. Does anything of concern stand out to you?
```{r summary table}
describe(Dataset1)
summary_table <- describe(Dataset1[ ,c("LEBF20052","maxINFM20052","GDPPCUS2005", "HXPC2005","PctHXPUB2005",
"TotFertRate2005","AdolFertRt2005","FtoMPrimaryEnrl2005ONY",
"PctUrb2005", "Sanitation2005ONY","ImprWaterUrb2005ONY",
"ImpWatRur2005ONY", "FLFPR2005", "PctPop0142005", "PctPop65Pl2005")],
fast=TRUE)
summary_table <- summary_table[ ,-(1), drop = FALSE] #drop first column which corresponds to variable number in dataframe
summary_table
```
What raises concern with the outputs of the table is the large standard deviation and standard error for GDPPC and HXPC. This indicates that large spread of data around the mean and that the sample mean is not a good representation of the population.
--------------------------------------------------------------------------------
Question 4 - Regress LEBF on HXPC. Report the coefficients,standard errors, confidence intervals, p-values, R2, and sample size in a regression table. Interpret the table, noting the economic and statistical significance of the relationship. What is the association between a 1,000-unit increase in HXPC and LEBF? (Note that HXPC is measured in dollars.)
```{r first regression}
first_regression <- lm(LEBF20052~HXPC2005, Dataset1)
Regression_table1 <- msummary(first_regression, fmt = "%.4f",
stars=c('*' = .1, '**' = .05, '***' = .01),
statistic = c("conf.int",  "standard error = {std.error}",
"p-value = {p.value}")
)
Regression_table1
```
The table shows that Health expenditure has a significant effect on the life expectancy at birth for females. This is economically significant because it shows that investment in Health expenditure is related to a longer life expectancy for females at birth. This provides the government with information on potential resource allocation. The table shows that a one unit increase in health expenditure leads to a 0.0040 increase in the life expectancy of females at birth. If there was a $1000 increase in health expenditure the life expectancy of females at birth would increase by 4 units.
--------------------------------------------------------------------------------
Question 5 - Now regress LEBF on HXPC and GDPPC. Discuss the results of this regression relative to those from (4).
```{r Question No.5}
second_regression <- lm(LEBF20052~HXPC2005+GDPPCUS2005, Dataset1)
Regression_table2 <- msummary(list(first_regression,second_regression),
stars=c('*' = .1, '**' = .05, '***' = .01),
fmt = "%.4f",  statistic = c("conf.int",  "standard error
={std.error}",
"p-value = {p.value}"))
Regression_table2
```
The results of this regression show that when GDPPC is controlled for the effects
of HXPC are no longer statistically significant.The regression coefficient of HXPC becomes reduced in this new model, going from 0.004 to -0.001. The new coefficient for Health Expenditure is negative which would mean increased health expenditure is related to a reduction in life expectancy. We would generally expect an increase in health expenditure to be positively related to life expectancy, thus it makes sense why these results are no longer significant. The regression coefficient for GDP is 0.0005 and is statistically significant. This provides indication that there is a relationship between GDDPC and LEBF. For every one unit increase in GDP, LEBF will increase by 0.0005 units.
--------------------------------------------------------------------------------
Question 6 - Do you recommend a nonlinear transformation for either GDPPC, HXPC, or LEBF? If so, defend your choice and repeat the regression in (5) with the appropriate transformations. Interpret how your results have changed.
```{r Question No.6}
# Visually, look and see how well the observations fit the linear model
vtable(Dataset1)
attach(Dataset1)
hist(GDPPCUS2005)
hist(log(GDPPCUS2005))
hist(HXPC2005)
hist(log(HXPC2005))
ggplot(Dataset1, aes(x = HXPC2005, y = LEBF20052)) +
geom_point() +
stat_smooth(method = "lm", col = "red")
ggplot(Dataset1, aes(x = GDPPCUS2005, y = LEBF20052)) +
geom_point() +
stat_smooth(method = "lm", col = "red")
ggplot(Dataset1, aes(x = log(HXPC2005), y = LEBF20052)) +
geom_point() +
stat_smooth(method = "lm", col = "red")
ggplot(Dataset1, aes(x = log(GDPPCUS2005), y = LEBF20052)) +
geom_point() +
stat_smooth(method = "lm", col = "red")
```
The histograms of GDPPC and HXPC show that their data is rightly skewed and thus is likely to produce outliers. Additionally, looking at how well the observations fit around the regression line, we can see that the plots for both GDDPC and HXPC do not fit the regression line well. We recommend, doing a log transformation of these two variables because it will make the big observations in the data smaller and thus closer to the other variables in the data. This will help reduce the skew and help the model behave more effectively.The histograms of the log transformed variables show that when a log transformation is performed the data looks approximately fits a normal distribution which is helpful for drawing statistical inference. The plots for both log(GDDPC) and log(HXPC) also add support for the use of a log transformation. These plots show that the regression line fits the data a lot better and thus a non-linear transformation should be used.
```{r Question No. 6 pt 2}
third_regression <- lm(LEBF20052~log(HXPC2005)+log(GDPPCUS2005), Dataset1)
Regression_table3 <- msummary(list("non_log_reg" =second_regression,
"log_reg"= third_regression),
stars=c('*' = .1, '**' = .05, '***' = .01),
fmt = "%.4f",  statistic = c("conf.int",  "standard error
={std.error}",
"p-value = {p.value}"))
Regression_table3
```
Applying a non-linear transformation is necessary in order to reduce the extreme effects of outliers on our regression outputs. Log transforming the predictor variables in the regression model is one way in which we can achieve this. It makes sense to log transform the independent variables in the model by applying a natural log, as we can directly compare the regression output to those in (5) as the output stays in the same units. We are also able to apply the natural log as we do not have any zero-valued data.
By log transforming both HXPC and GDPPC we are provided with new regression coefficients. The regression coefficient of log(HXPC2005) is now 0.8886 which is substantially different than the non-log'd coefficient -0.0014. The new regression coefficient indicates that for every 10% increase in HXPC (when GDPPC is held constant), the LEBF will increase by 0.089. Intuitively, life expectancy should increase as there is more health expenditure within a country so these new results make more sense which speaks to the value of transforming the HXPC variable. The results however remain statisically insignificant. The regression coefficient for log(GDPPCUS2005) also changed to 4.114 which is an increase from the non-log'd coefficient of 0.0005. Both coefficients were deemed as statistically significant. The new regression coefficient indicates that for every 10% increase in GDP (when HXPC is held constant) there is a 0.4114 increase in life expectancy at birth for females. We can also see that the R^2 value has increased from 0.252 to 0.493 which indicates that the new regression line fits better to the data.
--------------------------------------------------------------------------------
Question 7 - How might these results differ by geography? Create a variable that assigns each observation to a geographic region (e.g., continent) and report a regression that builds on (6) by including the appropriate dummy variables. Interpret your results.
```{r regression with continents}
Dataset1$continent <- countrycode(sourcevar = Dataset1[,"Country"],
origin = "country.name",
destination = "continent")
view(Dataset1)
unique(Dataset1[c("continent")])
Dataset1 <- dummy_cols(Dataset1, select_columns = "continent") #used fast dummies to make dummy variables
continet_regression <- lm(LEBF20052~log(HXPC2005)+log(GDPPCUS2005) + continent_Africa +
continent_Americas + continent_Asia + continent_Europe
+ continent_Oceania, Dataset1)
#all dummy variables are in the regression - to let R decide which one to drop
Regression_table4 <- msummary(list("continent_reg"=continet_regression),
stars=c('*' = .1, '**' = .05, '***' = .01),
fmt = 5,  statistic = c("conf.int",  "standard error = {std.error}",
"p-value = {p.value}"))
Regression_table4
```
When including variables that assign each observation to a geographic region (and holding them constant), the relationship between HXPC and LEBF shows a 10% increase in HXPC is associated with a 0.008624 increase in LEBF, but this remains not statistically significant. The relationship between GDPPC and LEBF sees a 10% increase in GDDPC significantly associated with a 0.331976 increase in LEBF which is a slight decrease from the output reported in (6) where a 10% increase in GDDPC was significantly associated with a 0.4114 increase in LEBF.The association between the continent of Africa and LEBF (when controlling for HXPC, GDDPC, and other continents) is statistically significant and negative (vs. Oceania). Conversely, the other continents have a positive relationship with LEBF although it is not statistically significant. The R^2 for the model of 0.641 shows good model fit and indicates that the variables in the model are able to adequately account for the variation in LEBF.
--------------------------------------------------------------------------------
Question 8 - Finally, include an interaction term between HXPC and the indicator for African countries. What are you measuring with this interaction, and why might it be meaningful? Interpret the results of this coefficient.
```{r interaction regression}
interaction_regression <- lm(LEBF20052~log(HXPC2005)
+log(GDPPCUS2005)
+ continent_Africa
+ continent_Americas
+ continent_Asia
+ continent_Europe
+ continent_Oceania
+ log(HXPC2005):continent_Africa, Dataset1)
Regression_table5 <- msummary(list("interaction_reg"=interaction_regression),
stars=c('*' = .1, '**' = .05, '***' = .01),
fmt = 5,  statistic = c("conf.int",
"standard error = {std.error}",
"p-value = {p.value}"))
Regression_table5
```
With this interaction term we are measuring effect of HXPC specific to African countries. It might be meaningful to include this interaction term as it would allow us to determine how much stronger the effect of HXPC on LEBF is when the continent is Africa. The interaction term shows that for an African country, a 10% increase in health expenditure increases LEBF by 0.038514, however this relationship is not statistically significant.
When we look at the interaction between HXPC and the continent of Africa we see that the regression coefficient is 0.32, which indicates that for Africa, a 10% increase in health expenditure leads to an additional 0.032 life years on top of the general effect of health expenditure which would be 0.0066 life years for ever 10% increase in health expenditure. It is the combination of these two values that lets us identify that the cumulative effect of a 10% increase in health expenditure in Africa increases LEBF by 0.0385.
--------------------------------------------------------------------------------
Question 9 - Why is establishing the causal relationship between GDDPC, HXPC, and LEBF difficult in a simple regression such as this? If possible, provide one key figure that highlights an identification problem in this scenario. (Note that there are multiple possible answers for this problem; the goal is to think critically about the causal identification.)
```{r Question No.9}
q.9_regression.1 <- lm(LEBF20052~log(GDPPCUS2005), Dataset1)
q.9_regression.2 <- lm(LEBF20052~log(HXPC2005), Dataset1)
q.9_table <- msummary(list("LEBF_log(GDP)"=q.9_regression.1, "LEBF_log(HXPC)"=q.9_regression.2,
"LEBF_log(GDP)+log(HXPC)"= third_regression), fmt = "%.4f",
stars=c('*' = .1, '**' = .05, '***' = .01),
statistic = c("conf.int", "standard error = {std.error}",
"p-value = {p.value}"))
q.9_table
```
Establishing the causal relationship between GDDPC, HXPC, and LEBF in a simple regression such as this is difficult as there are many sources of variation that can complicate the causal pathway between these three variables (as shown in the DAG in Question 1). This relationship is further complicated when the relationship between continents (countries) are taken into consideration, as from our DAG they drive much of the variation seen within GDPPC and subsequently HXPC.From our DAG we can see a substantial number of different sources of variation coming from GDPPC (which ultimately includes HXPC). Both GDPPC and HXPC account for a substantial amount of variation in LEBF (when we examine R^2 in the regressions for both explanatory variables on LEBF), but using theory (seen through our DAG) we can realize that there are other contributors within the data generating process that are not captured through this relationship. Additionally, when constructing a bivariate model, we can see that the R^2 is still lower than the univariate model examining LEBF and GDPPC, indicating that the inclusion of HXPC does not help in identifying what is associated with variation in LEBF. Moreover in this bivariate model, when controlling for GDPPC, HXPC decreases substantially from the univariate (LEBF~HXPC) model and becomes statistically insignificant. This indicates that HXPC likely has a much smaller role to play in the relationship between HXPC, GDPPC, and LEBF when GDPPC is taken into consideration. From this, we can appreciate that the DAG can help us to form testable assumptions and the models can help us to check those assumptions (to a point), but ultimately unless we are certain that we have accounted for all the variation in our outcome (through the DAG we construct), we cannot establish a causal relationship between HXPC, GDPPC, and LEBF.
--------------------------------------------------------------------------------
Question 10 - What do your results from (9) and intuition suggest about the standard errors in your specification? Either justify the use of homoscedastic standard errors or implement a full specification with another, more robust method (e.g., heteroskedasticity-robust or clustered SEs). How does this change the results?
```{r Question No.10}
#plot the residuals vs fitted values
par(mfrow = c(2, 2))
plot(interaction_regression)
#plot shows heteroskedasticity may exist
#Breusch-Pagan test for heteroskedasticity
lmtest::bptest(interaction_regression)
#the bptest is showing that the we can reject the null that the residuals are distributed with equal variance (homoscedasticty) as there is a high test statistic (BP = 49.79) and a p value < 0.05, therefore we can reject the null that the residuals are homoscedastic
#specification using both robust corrections for standard errors and autocorrelation
se_corrections<- msummary(list("homoscedastic" = interaction_regression,
"robust_se" =interaction_regression),
stars=c('*' = .1, '**' = .05, '***' = .01),
vcov =c("classical","robust"),
fmt = "%.2f", statistic = c("conf.int", "standard error = {std.error}",
"p-value = {p.value}"))
se_corrections
```
The results from question 9, and our intuition suggest that the standard errors in our specification are not homoscedastic. To check if it is accurate to use homoscedastic errors, a plot was created to look at the residual vs fitted values of our model that includes continents. We include continents to represent our theoretical assumptions outliine in our DAG, indicating that the variation of GDPPC and HXPC begins from country (continental) variation. In this residual plot, we can see that the data is not randomly spread around the 0 line and thus indicative that the use of homoscedastic standard errors may not be best practice. To confirm what we saw in the residual vs fitted plot, we conducted a Breusch-Pagan test to test for heteroskedasticity in our model. The results of our Breusch-Pagan test displayed a high test statistic and a p value < 0.05. Therefore, we reject the null hypothesis and can confirm that this regression model violates the homoscedasticity assumption.
As a result of this plot and the Breusch-Pagan test, we implemented a correction for heteroskedasticity within our model. We did not implement a correction for autocorrelation as our data was not collected over multiple time periods so it would not make sense for autocorrelation to have an impact. When we used the heteroskedasticity-robust method for our standard errors we observed that the standard errors for log(HXPC) and log(GDPPC) increased from 1.45 and 1.50 to 2.06 and 2.28 respectively. The standard errors increasing indicates that our standard errors are becoming more accurate in describing th data. We can also see that within our data that the regression coefficient for log(GDPPC) is no longer significant after this method is employed. This result is expected because the larger standard error, 2.78, observed leads to a test statistic that is smaller. Smaller test statistics are associated with larger p-values thus the loss of significance of the regression coefficient for log(GDDPC) as a result of the heteroskedasticity-robust method employed makes sense.
#i didnt talk about the other standard errors for the continents - idk if i should but get some funky things that idk how to explain like standard errors going down not up haha
--------------------------------------------------------------------------------
```{r Citations, include=FALSE}
knitr::write_bib(file = 'packages.bib') # Constructs a citation file for all packages used in this assignment.
```
knitr::opts_chunk$set(echo = TRUE)
name <- Sys.info()
name[7]
### Load the packages we will need for this file ####
library(tidyverse) # load the installed package for each new session of R
library(broom) # helps for storing regression output
library(here) # for file organization
library(modelsummary) # For making regression tables
library(psych) # For creating summary statistics table
library(countrycode) # Used to get continents from countries
library(fastDummies) # To create dummy variables
library(vtable) #Shows information about the variables in your data set
library(lmtest) #A collection of tests, data sets, and examples for diagnostic checking in linear regression models
# set working directory
here()
assign_data <- read.csv("Dataset 1.csv") # assign data to an object
# Scan through dataframe to see variable class types
for(values in names(assign_data)){
print(names(assign_data[values]))
name <- names(assign_data[values])
print(class(assign_data[,values]))
}
# Convert all to character
assign_data[, 1:19] <- lapply(assign_data[, 1:19], as.character)
# Assign the NAs in dataset to NA
assign_data <- mutate_at(.tbl = assign_data,
.vars = vars(Country:PctPop65Pl2005),
.funs = ~case_when(. == "#N/A" ~ NA_character_,
. == "-" ~ NA_character_,
TRUE ~ .,
TRUE ~ as.character(NA_character_)))
# Save cleaned data
write.csv(assign_data, "Dataset 1_Clean.csv", row.names = FALSE)
# Read in cleaned data
Dataset1 <- read.csv("Dataset 1_Clean.csv")
# Convert to numerics (aside from Country)
Dataset1[, 2:19] <- lapply(Dataset1[, 2:19], as.numeric)
describe(Dataset1)
summary_table <- describe(Dataset1[ ,c("LEBF20052","maxINFM20052","GDPPCUS2005", "HXPC2005","PctHXPUB2005",
"TotFertRate2005","AdolFertRt2005","FtoMPrimaryEnrl2005ONY",
"PctUrb2005", "Sanitation2005ONY","ImprWaterUrb2005ONY",
"ImpWatRur2005ONY", "FLFPR2005", "PctPop0142005", "PctPop65Pl2005")],
fast=TRUE)
summary_table <- summary_table[ ,-(1), drop = FALSE] #drop first column which corresponds to variable number in dataframe
summary_table
first_regression <- lm(LEBF20052~HXPC2005, Dataset1)
Regression_table1 <- msummary(first_regression, fmt = "%.4f",
stars=c('*' = .1, '**' = .05, '***' = .01),
statistic = c("conf.int",  "standard error = {std.error}",
"p-value = {p.value}")
)
Regression_table1
second_regression <- lm(LEBF20052~HXPC2005+GDPPCUS2005, Dataset1)
Regression_table2 <- msummary(list(first_regression,second_regression),
stars=c('*' = .1, '**' = .05, '***' = .01),
fmt = "%.4f",  statistic = c("conf.int",  "standard error
={std.error}",
"p-value = {p.value}"))
Regression_table2
# Visually, look and see how well the observations fit the linear model
vtable(Dataset1)
attach(Dataset1)
hist(GDPPCUS2005)
hist(log(GDPPCUS2005))
hist(HXPC2005)
hist(log(HXPC2005))
ggplot(Dataset1, aes(x = HXPC2005, y = LEBF20052)) +
geom_point() +
stat_smooth(method = "lm", col = "red")
ggplot(Dataset1, aes(x = GDPPCUS2005, y = LEBF20052)) +
geom_point() +
stat_smooth(method = "lm", col = "red")
ggplot(Dataset1, aes(x = log(HXPC2005), y = LEBF20052)) +
geom_point() +
stat_smooth(method = "lm", col = "red")
ggplot(Dataset1, aes(x = log(GDPPCUS2005), y = LEBF20052)) +
geom_point() +
stat_smooth(method = "lm", col = "red")
third_regression <- lm(LEBF20052~log(HXPC2005)+log(GDPPCUS2005), Dataset1)
Regression_table3 <- msummary(list("non_log_reg" =second_regression,
"log_reg"= third_regression),
stars=c('*' = .1, '**' = .05, '***' = .01),
fmt = "%.4f",  statistic = c("conf.int",  "standard error
={std.error}",
"p-value = {p.value}"))
Regression_table3
Dataset1$continent <- countrycode(sourcevar = Dataset1[,"Country"],
origin = "country.name",
destination = "continent")
view(Dataset1)
unique(Dataset1[c("continent")])
Dataset1 <- dummy_cols(Dataset1, select_columns = "continent") #used fast dummies to make dummy variables
continet_regression <- lm(LEBF20052~log(HXPC2005)+log(GDPPCUS2005) + continent_Africa +
continent_Americas + continent_Asia + continent_Europe
+ continent_Oceania, Dataset1)
#all dummy variables are in the regression - to let R decide which one to drop
Regression_table4 <- msummary(list("continent_reg"=continet_regression),
stars=c('*' = .1, '**' = .05, '***' = .01),
fmt = 5,  statistic = c("conf.int",  "standard error = {std.error}",
"p-value = {p.value}"))
Regression_table4
interaction_regression <- lm(LEBF20052~log(HXPC2005)
+log(GDPPCUS2005)
+ continent_Africa
+ continent_Americas
+ continent_Asia
+ continent_Europe
+ continent_Oceania
+ log(HXPC2005):continent_Africa, Dataset1)
Regression_table5 <- msummary(list("interaction_reg"=interaction_regression),
stars=c('*' = .1, '**' = .05, '***' = .01),
fmt = 5,  statistic = c("conf.int",
"standard error = {std.error}",
"p-value = {p.value}"))
Regression_table5
q.9_regression.1 <- lm(LEBF20052~log(GDPPCUS2005), Dataset1)
q.9_regression.2 <- lm(LEBF20052~log(HXPC2005), Dataset1)
q.9_table <- msummary(list("LEBF_log(GDP)"=q.9_regression.1, "LEBF_log(HXPC)"=q.9_regression.2,
"LEBF_log(GDP)+log(HXPC)"= third_regression), fmt = "%.4f",
stars=c('*' = .1, '**' = .05, '***' = .01),
statistic = c("conf.int", "standard error = {std.error}",
"p-value = {p.value}"))
q.9_table
#plot the residuals vs fitted values
par(mfrow = c(2, 2))
plot(interaction_regression)
#plot shows heteroskedasticity may exist
#Breusch-Pagan test for heteroskedasticity
lmtest::bptest(interaction_regression)
#the bptest is showing that the we can reject the null that the residuals are distributed with equal variance (homoscedasticty) as there is a high test statistic (BP = 49.79) and a p value < 0.05, therefore we can reject the null that the residuals are homoscedastic
#specification using both robust corrections for standard errors and autocorrelation
se_corrections<- msummary(list("homoscedastic" = interaction_regression,
"robust_se" =interaction_regression),
stars=c('*' = .1, '**' = .05, '***' = .01),
vcov =c("classical","robust"),
fmt = "%.2f", statistic = c("conf.int", "standard error = {std.error}",
"p-value = {p.value}"))
se_corrections
knitr::write_bib(file = 'packages.bib') # Constructs a citation file for all packages used in this assignment.
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
