---
title: "Assignment 3"
author: "Husayn Jessa and Aidan Bodner"
date: "16/11/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load Packages
```{r Packages}
library(here)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(ggrepel)
library(ggtext)
library(showtext)
library(stringr)
library(fastDummies)
library(modelsummary)
library(lubridate)
library(gtsummary)
library(fixest)
library(quantreg)

```

# Data and Cleaning
```{r Data and Cleaning}
name <- Sys.info() 
name[7]

assign_data <- read.csv("a3_p1_sourdough_trends.csv")

```

# Question 1a
```{r Question 1a}

assign_data$date <- str_replace_all(assign_data$date, "T00:00:00Z", "")
assign_data$date <- as.factor(assign_data$date)
assign_data$date <- as.numeric(assign_data$date) 

q1_line_graph <- ggplot(assign_data, (aes(x = date, y = hits, group = keyword))) + 
  geom_line(aes(color = keyword)) + theme_linedraw() + labs(x = "Date", y = "Hits") +
  geom_vline(xintercept = as.numeric(assign_data$date[75]), 
    color = "red") + geom_text(aes(x = as.numeric(date[120]), label="Start of Pandemic (2020-03-15)", y=90), color = "red") +
  scale_x_discrete(breaks = c("2020-01-01",
                              "2020-02-01",
                              "2020-03-01",
                              "2020-04-01",
                              "2020-05-01",
                              "2020-06-01",
                              "2020-07-01",
                              "2020-08-01"), labels = waiver())

q1_line_graph


#I wanted to make a line graph without converting our dates to numeric:

#create column new_date to change the date variable into dates
assign_data$date <- str_replace_all(assign_data$date, "T00:00:00Z", "")
assign_data$new_date <- ymd(assign_data$date)
class(assign_data$new_date)

hq1_line_graph <- ggplot(assign_data, aes(x = date, y = hits, colour = keyword, group = keyword)) +
                theme_classic()  + geom_vline(xintercept = assign_data$date[75], show.legend = FALSE) + annotate("text", x=assign_data$date[70], y=86, label="Start of Pandemic ", angle=90) + geom_line() +  scale_x_discrete(breaks = c("2020-01-01",
                              "2020-02-01",
                              "2020-03-01",
                              "2020-04-01",
                              "2020-05-01",
                              "2020-06-01",
                              "2020-07-01",
                              "2020-08-01")) + ggtitle("Interest in Food Types Over Time")

hq1_line_graph

# Look at plot at pre-treatment, to gauge trend lines and similarities or differences: 

hq1_line_graph + coord_cartesian(xlim= c(0,76))
#we can see before the start of the pandemic sourdough looked similar to cereal but did not look anything like soup or sandwich so considering droping thos



```
Based on the graph we created, when the pandemic started there were temporary effects on the popularity of sourdough bread. From the graph we can see that after the start of the pandemic the hits for sourdough increased to a certain point then started to decline back to pre pandemic levels. For cereal and sandwich respectively, their popularity levels remained relatively the same before and after the pandemic started. Soup was experiencing a decline in popularity before the pandemic began and then experienced a small increase in popularity when the pandemic began but soon after continued its descent in popularity. As sourdough is our variable of interest, it was interesting to see that the levels in popularity increased for a few months after the pandemic began however it appears that as people got used to living in the pandemic, they returned to their pre pandemic habits. 

Sourdough bread will be considered our Treated group and thus when looking at the other foods as controls, some are more appropriate than others. When the graph was zoomed in to look at the popularity trends before the pandemic began it can be noted that the trend of sourdough bread looks most like that of cereal. They both experience low peaks and dips in their popularity trend line and have very similar locations at where these peaks and dips occur. Sourdough's trend line and sandwich's trend line do not look as similar, as sandwich popularity appears to have much more changes and sharper dips and peaks than that of sourdough's. The trend line for soup is the most distinct from that of sourdoughs as it has large variation in popularity over time and it has a clear downard trend in popularity. Based on the pre-pandemic trend lines, it appears that cereal would serve as the best control group for sourdough as the two trend lines look very similar and thus without any impactful events (such as a pandemic) it could be predicted that their trend lines would look similar if extrapolated. 


# Question 1b
```{r Question 1b}

# Create Treated Variable for sourdough https://www.princeton.edu/~otorres/DID101R.pdf

regdata <- assign_data %>% mutate(treated = ifelse(keyword == "sourdough", 1, 0)) 

# Create a dummy variable to indicate the time when the treatment started. 

regdata <- regdata %>% mutate(pandemic_begun = ifelse(date >= "2020-03-15", 1, 0))

# Create interaction variable between treated and beginning of pandemic

regdata <- regdata %>% mutate(interaction = treated*pandemic_begun)

# Statistical tests: are the two groups' trends different?
pretrend_test <- lm(hits ~ treated + pandemic_begun + interaction, data=regdata)
msummary(list(pretrend_test),
         vcov=c(rep("robust",1)),
         stars=c('*' = .1, '**' = .05, '***' = .01)) # Interpret each coefficient here

# Dropping Control Groups

# Dropping soup and sandwich from data 
new_regdata <- regdata %>% filter(keyword %in% c("sourdough","cereal"))

unique(new_regdata$keyword)

new_pretrend_test <- lm(hits ~ treated + pandemic_begun + interaction, data=new_regdata)

msummary(list("All control groups" =pretrend_test,"Dropped Control Groups"= new_pretrend_test),
         vcov=c(rep("robust",1)),
         stars=c('*' = .1, '**' = .05, '***' = .01)) # Interpret each coefficient here

#i think this is how you do it but im not sure


```
The regression results for either the model where we keep all the control groups or the model where we drop soup and sandwich are not concerning. If we hypothesize that sourdough popularity only begun at the pandemic start date (2020-03-15), then it would be likely that any given search for sourdough would result in an increase in hits. Similarly, it is difficult to say that internet traffic (hits), for any of the dates after the pandemic start would be more or less than that before the pandemic. Therefore, the negative coefficients for treated and pandemic_begun variables are not concerning. The interaction term makes sense given our hypothesis as it indicates a positive change in hits for sourdough after the pandemic start date compared to before. 

When we drop the other control variables, our standard errors decrease and our R^2 increases, indicating a more precise estimate of the number of hits and better model fit. For this reasons as well, we are not concerned.


# Question 1c
```{r Question 1c}


#create a relative_month variable by shifting the "date" variable back 15 days (so that the treatment day is the first day of the month)  and then taking the month of the resulting date 
new_regdata <- new_regdata %>% mutate(relative_month = new_date - 14)     
#shift date back 15 days, used new_date instead of date as date is a character while new date is the class date
# I used 14 so that the treatment day is may first rather than February 29  


#create an "After" variable equal to 1/0 or True/False if the date is March 15 or afterwards 

new_regdata <- new_regdata %>% mutate(After = ifelse(relative_month >= "2020-03-15", 1, 0 )) 


#illustrate how the values of "relative_month" line up with "date" and subtract a number from "relative_month" so the last period just before treatment (Feb 16 - Mar 14)  is 0. (Also, change the Jan 1 -14 month so it's one less than the Jan 15- Feb 14 month) 

relative_month_date_illustration <- new_regdata %>% select(relative_month, new_date)

#table with relative month and new date next to each other 
relative_month_date_illustration <- relative_month_date_illustration %>% mutate(diff_bt_date = difftime(new_date, relative_month, units = "days"))

plot(relative_month_date_illustration$diff_bt_date)

q_c_table <- tibble(relative_month_date_illustration)


```

# Question 1d
```{r Question 1d}

#Use two-way fixed effects to estimate the difference-in-difference estimate of the effect of lockdown on sourdough popularity. What are your two fixed effects? 

fixed_effects_model <- feols(hits ~ interaction | pandemic_begun + treated, data = new_regdata)
#idk what fixed effects we have to include here also not sure if we include all 3 of these variables in the ols  (maybe include keyword as the fixed effect but that introduces colineaarity with treated)
fixed_effects_model


#report and interper results, with standard errors clustered at keyword level 

msummary(fixed_effects_model, vcov= ~keyword, fmt = '%#.14f') #is this how you cluster by keyword level haha
```
Our two fixed effects are the date for the beginning of the pandemic (pandemic_begun) and whether the keyword is sourdough (treated). The model shows that for keyword searches of sourdough at the start or after the pandemic had 8.1 more hits than its controls. Moreover, with our standard errors clustered at the "keyword" level (0.00000000000008), our estimate of the interaction is extremely precised. This indicates that sourdough popularity was in fact higher once the pandemic began however the results do not appear as significant. 


#PART 3 Quantile and Nonparametric Regression
```{r Data and Cleaning}
name <- Sys.info() 
name[7]

here()
load("/Users/husaynjessa/Documents/GitHub/HAD5744/Assignment 3/a3_p3_medicare.RData")

```

# Question 3a
```{r Question 3a}
# Construct Medicare coverage variable (1 = covered; 0 = not covered)
mydata <- mydata %>% mutate(medicare = ifelse(agex >= 65, 1, 0)) 

medicare <- mydata[mydata$medicare == 1, ]
no_medicare <- mydata[mydata$medicare == 0, ]

# View distribution of out-of-pocket medical spending for spending after Medicare coverage
ggplot(medicare, aes(x=totexp)) + geom_histogram() + geom_histogram(bins = 100) + xlim(0, 100000) + ylim(0, 20000)

# View distribution of out-of-pocket medical spending for spending before Medicare coverage
ggplot(no_medicare, aes(x=totexp)) + geom_histogram() + geom_histogram(bins = 100) + xlim(0, 25000) + ylim(0, 100000)

```
Examining the two histograms shows that those with Medicare incur less out-of pocket expenses than those with no Medicare coverage. This shows us that, medicare may provide financial protection however further analysis is required to know if this is actually the case.  While these histograms give us a good starting point to believe that this may be the case, we cannot claim that this is a causal relationship as we have not accounted for unexplained variation between these two groups.


# Question 3b
```{r Question 3b}
naive_ols <- lm(totexp ~ medicare + sex + educyr + racex + faminc, data = mydata, weights = perwt) # We will have to figure out what we want to control for
msummary(naive_ols)

attach(mydata)

```
The naive OLS estimate for the effect of Medicare on total expenditures is that being on Medicare increases an individuals total spending by $5644.32.This is counter-intuitive to what we would have expected as in part A the distribution of total spending made it appear that Medicare offered financial protection.

# Question 3c

```{r Question 3c}

quant_reg <- rq(totexp ~ medicare + educyr + sex + racex + faminc, tau= c(0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75,
            0.8, 0.85, 0.9, 0.95, 0.1), data = mydata, weights = perwt)


quant_reg_summary <- summary(quant_reg)



mytau <- rep(NA,20) # empty vector: quantiles
coefs <- rep(NA, 20) # empty vector: coefficients
lb <- rep(NA, 20) # empty vector: 95% LB
ub <- rep(NA, 20) # empty vector: 95% UB
for (t in 1:20) {
  mytau[t] <- t/20 # indicate which decile I am using
  print(paste0("Considering quantile ",mytau[t],sep=" "))
quantreg <- rq(totexp ~ medicare + educyr + sex + racex + faminc, data = mydata, weights = perwt,
              tau = mytau[t]) # tau ranges from 0 to 1
  coefs[t] <- quantreg$coefficients[2]
  quantsummary <- summary(quantreg, se = "iid")
  lb[t] <- coefs[t]-1.96*quantsummary$coefficients[2,2]
  ub[t] <- coefs[t]+1.96*quantsummary$coefficients[2,2]  
}

msummary(quantreg)

plotdata <- read.csv("Dataframe_for_Question3c")

# Look at distribution of coefficients 
plotdata %>%ggplot(aes(x=mytau)) +
  geom_point(aes(y=coefs),size=2,color='blue') + 
  geom_errorbar(aes(ymin = lb, ymax = ub),width=0.03) + 
  scale_y_continuous(labels=scales::dollar_format()) + 
  scale_x_binned(n.breaks = 20, limits = c(0.05, 1.00)) +
  theme_classic() + labs(x="Quantile", y="Estimated Marginal Effect of Medicare Enrolment on Health Spending")

# Look to see if the dropping people who contribute negative costs makes the plot more readable
plotdata %>% filter(mytau < 0.95) %>% ggplot(aes(x=mytau)) +
  geom_point(aes(y=coefs),size=2,color='blue') + 
  geom_errorbar(aes(ymin = lb, ymax = ub),width=0.03) + 
  scale_y_continuous(labels=scales::dollar_format()) + 
  scale_x_binned(n.breaks = 20, limits = c(0.05, 1.00)) +
  theme_classic() + labs(x="Quantile", y="Estimated Marginal Effect of Medicare Enrolment on Health Spending")
# The plot is not that much more readable and even takes away from the story of medicare enrolment on health spending as negative spending is still an important component to understanding what medicare does to health spending.

```
We can see from the plots of spending quantiles that as the quantile increases, so does health expenditure. This up until the twentieth quantile, when medicare enrollment causes negative spending. This may be because this quantile is composed of superutilizers who cost the system the most and therefore may activate a more substantial tier of medicare that covers more expenses for this group. We can also see from our plot that our coefficients have non-overlaping confidence intervals indicating that the level changes between each quantile is significantly different from one another.


# Question 3d
```{r Question 3d}

# Construct a figure of coefficients across distribution
myquants <- quantile(mydata$totexp,probs=c(0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75,
            0.8, 0.85, 0.9, 0.95, 0.1), na.rm=TRUE)
plotdata %>% mutate(coefs = coefs / myquants, 
                    lb = lb / myquants, 
                    ub = ub / myquants) %>% 
  ggplot(aes(x=mytau))+
  geom_point(aes(y=coefs),size=2,color='blue') + 
  geom_errorbar(aes(ymin = lb, ymax = ub),width=0.03) + 
  geom_hline(yintercept=1,color='red',linetype='dashed') + 
  theme_classic() + labs(x="Quantile", y="Estimated Marginal Effect of Medicare Enrolment on Health Spending")
```
This scaling changes the interpretation as it allows us to determine where the largest changes in health spending occur between quantiles. Here, we can see that the largest change occurs between the 25th and 30th quantile, where there is an approximately an 8 times reduction in health spending. In this instance, medicare coverage makes a very large difference in health spending, as it changes individuals from being relatively lower spending to increasing their spending by approximately 26 times.


# Question 3e
```{r Question 3e}

library(binsreg) # For binscatters
library(nprobust) # Local linear regression

# Check this site for helpful coding tips for nonparametric techniques: https://nppackages.github.io/
# Binscatter: relationship between income and spending
mydata%>% ggplot(aes(x=faminc ,y=totexp)) + 
  geom_point() + 
  theme_classic() + 
  scale_y_continuous(labels=scales::dollar_format()) + 
  labs(x="faminc",y="Total Health Expenditures") # This is hard to interpret

# Omit missing observations on educyr
mydata <- mydata[!is.na(mydata$educyr), ]

# Let's bin the data more
binsreg(y=totexp,x=faminc,by=medicare,
        w= ~ sex + educyr + factor(racex),
        data=mydata,
        plotxrange = c(0, 250000),                               
        line=c(3,3)) # If we want any confidence intervals on points



```
The difference between the two curves begins to converge as family income increases. We can see at the lower end of family income (approaching zero) that the differences in total expenditure are greatest between people who receive and do not receive medicare. When family income increases the curves begin to converge which suggests that regardless of medicare coverage people who earn more will incur less cost on the healthcare system. This also suggests that as lower income individuals will require more healthcare services, being enrolled in medicare can be very beneficial in ensuring access to care. 

# Question 3f 

The results from (e) imply that the use of nonparametric regression could be a valid approach when working with this data. From (e), we could identify the trends between the total expenditure on health and how that related to family income distribution. We were also able to identify how being old enough for medicare effects the trends between those two variables. The shape of the relationship appears to provide a clear story about how income distribution influences total expenditure for both the medicare and non-medicare group and as a result it appears to provide valuable insight into the data. A framework that may be used to recover a nonparametric conditional density estimator of the effect of Medicare coverage over the income distribution is through using the kernel based density estimator. For this to be a valid framework, the kernel function must be symmetrical. The symmetric property of kernel functions allow for the maximum value of the function to be in the middle of the curve created by the function. Second the area under the curve of the kernel function must also equal 1. Finally the value of the kernel function can't be negative. To ensure the data fits appropriately with the kernel function, an optimal bandwith value is also required. A low value for bandwith estimates the density value with lots of variance while a high bandwith value creates large amount of bias. The choice of kernal size also is important as too large value leads to boundaries between classes less distinct but allows for a reduction of the noise effect on classification while a too small kernal size has the opposite effects If all these conditions are met then it is possible to obtain a conditional density estimator for our data using this method. 

https://medium.com/analytics-vidhya/kernel-density-estimation-kernel-construction-and-bandwidth-optimization-using-maximum-b1dfce127073 <- information on kernel density estimation was obtained here

# Question 3g

Medicare protects low-income people 65 years or older from the risk of high out-of-pocket health expenditures as well as eliminating a financial barrier in accessing care. We can see this in (e), where there is a substantial difference in total expenditure, this is primarily among low-income individuals. In e we can see that individuals on medicare have larger amounts of total expenditure and this speaks to why medicare is necessary for these groups. We know that aging leads to increased health costs and that low income people have worser health outcomes than higher income people. As a result, Medicare is useful to protect these individuals from catastrophic risk. 
We can also see that medicare coverage has implications for substantially lowering the differences in health spending between quantiles as seen in (d). 
The distribution amongst quantiles in (c) showed that as quantiles increased the marginal effect of medicare on health spending increased as well. This indicates that medicare has a greater effect with the more health services someone consumes as they recieve more financial protection. 
Potential open back doors: As we see large differences in quantiles on total expenditure in (d), a potential open back door that may be influencing this result is health status. Regardless of medicare coverage, health status will increase health expenditure. Another open back door that may be influencing the results is that, some people may have more than one insurance plan that covers the costs of the health spending and as such medicare may not be the only insurance plan members in our data set have. Thus the distributional effect of medicare as a form of risk protection may not be the true effect if the role of other insurance plans is not closed.  
Policy Implications: Our findings point to the need for policymakers to give greater public insurance coverage to low-income people as our findings indicate that this group contributes to substantial health expenditure when covered by medicare compared to when not covered by medicare. Moreover, the gaps in expediture quantiles when considering medicare status shows substantial differences between the 25th and 30th quantile of people who contribute to health expenditure indicating a stark disparity.







